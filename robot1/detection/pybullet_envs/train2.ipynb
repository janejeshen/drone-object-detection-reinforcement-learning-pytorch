{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacc4c17-2522-471f-82ad-a2aaf50b8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation import ArmEnv\n",
    "from wrapper import GymWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fde414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmPolicy(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ArmPolicy, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ArmDataset(IterableDataset):\n",
    "    def __init__(self, env):\n",
    "        super(ArmDataset, self).__init__()\n",
    "        self.env = env\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            observation = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.env.action_space.sample()\n",
    "                next_observation, reward, done, _ = self.env.step(action)\n",
    "                yield observation, action, reward\n",
    "                observation = next_observation\n",
    "\n",
    "\n",
    "def ppo_update(policy, optimizer, states, actions, rewards, clip_param=0.2, epochs=10, batch_size=64):\n",
    "    dataset = list(zip(states, actions, rewards))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            batch_states, batch_actions, batch_rewards = batch\n",
    "\n",
    "            logits = policy(batch_states)\n",
    "            dist = Categorical(logits=logits)\n",
    "            log_probs = dist.log_prob(batch_actions)\n",
    "\n",
    "            old_logits = policy(batch_states).detach()\n",
    "            old_dist = Categorical(logits=old_logits)\n",
    "            old_log_probs = old_dist.log_prob(batch_actions)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * batch_rewards\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * batch_rewards\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def train(env, num_episodes=1000, save_path=\"trained_model.pth\"):\n",
    "    if len(env.observation_space.shape) == 1:\n",
    "        input_size = env.observation_space.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space shape.\")\n",
    "\n",
    "    output_size = env.action_space.shape[0]\n",
    "\n",
    "    policy = ArmPolicy(input_size, output_size)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "\n",
    "    dataset = ArmDataset(env)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "        for observation, action, reward in dataset:\n",
    "            states.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if len(states) >= 64:\n",
    "                ppo_update(policy, optimizer, states, actions, rewards)\n",
    "                states, actions, rewards = [], [], []\n",
    "\n",
    "            if len(states) >= 1000:\n",
    "                break\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            total_reward = evaluate(policy, env)\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "        # Real-time training progress\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}/{num_episodes}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(policy.state_dict(), save_path)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def evaluate(policy, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logits = policy(torch.tensor(observation, dtype=torch.float32))\n",
    "                action = Categorical(logits=logits).sample().numpy()\n",
    "\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            observation = next_observation\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "    return avg_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e0555c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Only one local in-process GUI/GUI_SERVER connection allowed. Use DIRECT connection mode or start a separate GUI physics server (ExampleBrowser, App_SharedMemoryPhysics_GUI, App_SharedMemoryPhysics_VR) and connect over SHARED_MEMORY, UDP or TCP instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create the environment\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m ArmEnv()\n\u001b[0;32m      3\u001b[0m wrapped_env \u001b[39m=\u001b[39m GymWrapper(env)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Set a random seed for reproducibility\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\drone detection\\1_robot1\\robot1\\detection\\pybullet_envs\\simulation.py:12\u001b[0m, in \u001b[0;36mArmEnv.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     p\u001b[39m.\u001b[39;49mconnect(p\u001b[39m.\u001b[39;49mGUI)\n\u001b[0;32m     13\u001b[0m     p\u001b[39m.\u001b[39msetAdditionalSearchPath(pybullet_data\u001b[39m.\u001b[39mgetDataPath())\n\u001b[0;32m     14\u001b[0m     p\u001b[39m.\u001b[39msetGravity(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m9.8\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: Only one local in-process GUI/GUI_SERVER connection allowed. Use DIRECT connection mode or start a separate GUI physics server (ExampleBrowser, App_SharedMemoryPhysics_GUI, App_SharedMemoryPhysics_VR) and connect over SHARED_MEMORY, UDP or TCP instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the environment\n",
    "env = ArmEnv()\n",
    "wrapped_env = GymWrapper(env)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# '''\n",
    "# # Train the agent and save the model\n",
    "# train(env, save_path=\"trained_model.pth\")\n",
    "# print(\"Training completed and model saved.\")\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29f10bc-d810-41ba-8ff6-eb35b91b991c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO\n\u001b[0;32m      4\u001b[0m \u001b[39m# Define the PPO agent\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, wrapped_env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "# Define the PPO agent\n",
    "model = PPO(\"MlpPolicy\", wrapped_env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the trained agent\n",
    "model.save(\"ppo_arm\")\n",
    "\n",
    "# Load the trained agent\n",
    "loaded_model = PPO.load(\"ppo_arm\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "total_reward = 0\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = loaded_model.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
