{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacc4c17-2522-471f-82ad-a2aaf50b8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulationworkcopy import ArmEnv\n",
    "from wrapper import GymWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import numpy as np\n",
    "from simulationworkcopy import ObjectDetector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fde414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmPolicy(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ArmPolicy, self).__init__()\n",
    "\n",
    "        input_size = 307209  # or input_size = (307204,) \n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.object_detector = ObjectDetector()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def detect_object(self, x):\n",
    "        return self.object_detector.detect(x)  # Call object detection component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca5cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmDataset(IterableDataset):\n",
    "    def __init__(self, env):\n",
    "        super(ArmDataset, self).__init__()\n",
    "        self.env = env\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            observation = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.env.action_space.sample()\n",
    "                next_observation, reward, done, _ = self.env.step(action)\n",
    "                yield observation, action, reward\n",
    "                observation = next_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59a6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(policy, optimizer, states, actions, rewards, clip_param=0.2, epochs=10, batch_size=64):\n",
    "    dataset = list(zip(states, actions, rewards))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            batch_states, batch_actions, batch_rewards = batch\n",
    "\n",
    "            logits = policy(batch_states)\n",
    "            dist = Categorical(logits=logits)\n",
    "\n",
    "            log_probs = []\n",
    "            for action, logit in zip(batch_actions, logits):\n",
    "                action_dist = Categorical(logits=logit)\n",
    "                action = action.long()  # Convert action to integer\n",
    "                log_prob = action_dist.log_prob(action)\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "            log_probs = torch.stack(log_probs)\n",
    "\n",
    "            old_logits = policy(batch_states).detach()\n",
    "            old_dist = Categorical(logits=old_logits)\n",
    "\n",
    "            old_log_probs = []\n",
    "            for action, old_logit in zip(batch_actions, old_logits):\n",
    "                old_action_dist = Categorical(logits=old_logit)\n",
    "                action = action.long()  # Convert action to integer\n",
    "                old_log_prob = old_action_dist.log_prob(action)\n",
    "                old_log_probs.append(old_log_prob)\n",
    "\n",
    "            old_log_probs = torch.stack(old_log_probs)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            ratio = ratio.view(-1, 1)  # Reshape ratio to match batch_rewards shape\n",
    "            surr1 = ratio * batch_rewards\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * batch_rewards\n",
    "            surr1 = surr1.to(surr2.dtype)\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2579a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, env, num_eval_episodes=10):\n",
    "    total_rewards = []\n",
    "\n",
    "    for _ in range(num_eval_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logits = policy(torch.tensor(observation, dtype=torch.float32))\n",
    "            action = Categorical(logits=logits).sample().numpy()\n",
    "\n",
    "            # Choose action using the policy\n",
    "            #action = policy(torch.tensor(observation).float())\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Accumulate the total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "    #avg_reward = total_reward / num_eval_episodes\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec98656",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\User\\\\Documents\\\\drone detection\\\\robot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[39m# Train the agent and save the model\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m train(env\u001b[39m=\u001b[39;49mwrapped_env, save_path\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUser\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdrone detection\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mrobot\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     92\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining completed and model saved.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, num_episodes, save_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m object_detection_results \u001b[39m=\u001b[39m []  \u001b[39m# Store object detection results\u001b[39;00m\n\u001b[0;32m     26\u001b[0m start_episode \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 27\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(save_path) \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(save_path) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     start_episode \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\User\\\\Documents\\\\drone detection\\\\robot'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = \"C:/Users/User/Documents/drone detection/robot\"\n",
    "\n",
    "# Create the environment\n",
    "env = ArmEnv()\n",
    "wrapped_env = GymWrapper(env)\n",
    "\n",
    "def train(env, num_episodes=5, save_path = \"C:/Users/User/Documents/drone detection/robot/modelx.pth\"):\n",
    "    if len(env.observation_space.shape) == 1:\n",
    "        input_size = env.observation_space.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space shape.\")\n",
    "\n",
    "    output_size = env.action_space.shape[0]\n",
    "    \n",
    "    policy = ArmPolicy(input_size, output_size)\n",
    "\n",
    "    optimizer = optim.SGD(policy.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "\n",
    "    dataset = ArmDataset(env)\n",
    "\n",
    "    episode_rewards = []  # Store episode rewards\n",
    "    object_detection_results = []  # Store object detection results\n",
    "\n",
    "    start_episode = 0\n",
    "    checkpoint = torch.load(save_path) if os.path.exists(save_path) else None\n",
    "    if checkpoint is not None:\n",
    "        start_episode = checkpoint['episode'] + 1\n",
    "        policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode_rewards = checkpoint['episode_rewards']\n",
    "        object_detection_results = checkpoint['object_detection_results']\n",
    "\n",
    "    for episode in range(start_episode, num_episodes):\n",
    "        states, actions, rewards = [], [], []\n",
    "                \n",
    "        for observation, action, reward in dataset:\n",
    "            states.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if len(states) >= 5:\n",
    "                ppo_update(policy, optimizer, states, actions, rewards)\n",
    "                states, actions, rewards = [], [], []\n",
    "\n",
    "            if len(states) >= 5:\n",
    "                break\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            total_reward = evaluate(policy, env)\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "        # Real-time training progress\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode: {episode}/{num_episodes}\")\n",
    "\n",
    "        # Object detection and alert\n",
    "        detected_objects = policy.detect_object(states)\n",
    "        if \"Complex structure\" in detected_objects:\n",
    "            print(\"Alert: Complex structure detected!\")\n",
    "            object_detection_results.append(detected_objects)\n",
    "\n",
    "        # Save the model and training data after each episode\n",
    "        save_checkpoint(policy, optimizer, states, actions, rewards, episode, episode_rewards, object_detection_results, save_path)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Return the stored tensors\n",
    "    return episode_rewards, object_detection_results\n",
    "\n",
    "def save_checkpoint(policy, optimizer, states, actions, rewards, episode, episode_rewards, object_detection_results, save_path):\n",
    "    checkpoint = {\n",
    "        'episode': episode,\n",
    "        'policy_state_dict': policy.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'object_detection_results': object_detection_results\n",
    "    }\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved for episode {episode}.\")\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Train the agent and save the model\n",
    "train(env=wrapped_env, save_path=r\"C:\\Users\\User\\Documents\\drone detection\\robot\")\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(env, num_episodes=100, save_path=\"modelx.pth\"):\n",
    "#     if len(env.observation_space.shape) == 1:\n",
    "#         input_size = env.observation_space.shape[0]\n",
    "        \n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported observation space shape.\")\n",
    "\n",
    "#     output_size = env.action_space.shape[0]\n",
    "    \n",
    "#     policy = ArmPolicy(input_size, output_size)\n",
    "\n",
    "#     optimizer = optim.SGD(policy.parameters(), lr=0.01, momentum=0.9)\n",
    "#     optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "\n",
    "#     dataset = ArmDataset(env)\n",
    "\n",
    "#     episode_rewards = []  # Store episode rewards\n",
    "#     object_detection_results = []  # Store object detection results\n",
    "\n",
    "#     for episode in range(num_episodes):\n",
    "#         states, actions, rewards = [], [], []\n",
    "                \n",
    "#         for observation, action, reward in dataset:\n",
    "#             states.append(observation)\n",
    "#             actions.append(action)\n",
    "#             rewards.append(reward)\n",
    "\n",
    "#             if len(states) >= 64:\n",
    "#                 ppo_update(policy, optimizer, states, actions, rewards)\n",
    "#                 states, actions, rewards = [], [], []\n",
    "\n",
    "#             if len(states) >= 100:\n",
    "#                 break\n",
    "\n",
    "#         if episode % 10 == 0:\n",
    "#             total_reward = evaluate(policy, env)\n",
    "#             print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "#         # Real-time training progress\n",
    "#         if episode % 100 == 0:\n",
    "#             print(f\"Episode: {episode}/{num_episodes}\")\n",
    "\n",
    "#         # Object detection and alert\n",
    "#         detected_objects = policy.detect_object(states)\n",
    "#         if \"Complex structure\" in detected_objects:\n",
    "#             print(\"Alert: Complex structure detected!\")\n",
    "#             object_detection_results.append(detected_objects)\n",
    "\n",
    "#     # Save the trained model\n",
    "#     torch.save(policy.state_dict(), save_path)\n",
    "#     env.close()\n",
    "\n",
    "#      # Return the stored tensors\n",
    "#     return episode_rewards, object_detection_results\n",
    "\n",
    "# def evaluate(policy, env, num_eval_episodes=10):\n",
    "#     total_rewards = []\n",
    "\n",
    "#     for _ in range(num_eval_episodes):\n",
    "#         observation = env.reset()\n",
    "#         done = False\n",
    "\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             with torch.no_grad():\n",
    "#                 logits = policy(torch.tensor(observation, dtype=torch.float32))\n",
    "#             action = Categorical(logits=logits).sample().numpy()\n",
    "\n",
    "#             # Choose action using the policy\n",
    "#             #action = policy(torch.tensor(observation).float())\n",
    "\n",
    "#             # Take the action in the environment\n",
    "#             next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "#             # Accumulate the total reward\n",
    "#             total_reward += reward\n",
    "\n",
    "#             observation = next_observation\n",
    "\n",
    "#         total_rewards.append(total_reward)\n",
    "\n",
    "#     avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "#     #avg_reward = total_reward / num_eval_episodes\n",
    "#     return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0555c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the environment\n",
    "# env = ArmEnv()\n",
    "# wrapped_env = GymWrapper(env)\n",
    "\n",
    "# # Set a random seed for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f10bc-d810-41ba-8ff6-eb35b91b991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent and save the modeltrained_model.pth\n",
    "# train(env=wrapped_env, save_path='modelx.pth')\n",
    "# print(\"Training completed and model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained agent\n",
    "#model.save(\"ppo_arm\")\n",
    "\n",
    "# Load the trained agent\n",
    "# loaded_model = torch.load(\"modelx.pth\")\n",
    "\n",
    "# # Evaluate the trained agent\n",
    "# total_reward = 0\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "\n",
    "# while not done:\n",
    "#     action, _ = loaded_model.predict(obs)\n",
    "#     obs, reward, done, _ = env.step(action)\n",
    "#     total_reward += reward\n",
    "\n",
    "\n",
    "# print(\"Total reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
